---
output: 
  stevetemplates::article:
    fig_caption: true
vignette: >
  %\VignetteIndexEntry{An Introduction to mixR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
#bibliography: master.bib
biblio-style: apsr
title: "An Introduction to mixR"
#thanks: "Replication files are available on the author's Github account (http://github.com/svmiller). **Current version**: `r format(Sys.time(), '%B %d, %Y')`; **Corresponding author**: svmille@clemson.edu."
author:
- name: Youjiao Yu
  affiliation: Baylor University
abstract: "The package **mixR** performs maximum likelihood estimation for finite mixture models for families including Normal, Weibull, Gamma and Lognormal via EM algorithm. It also conducts model selection by using Bayesian Information Criterion (BIC) or bootstrap likelihood ratio test (LRT). The data used for mixture model fitting can be in the form of raw data or binned data. The model fitting is accelerated by using R package **Rcpp**."
keywords: "mixture models, EM algorithm, model selection"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
endnote: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE,
                      message=FALSE, warning=FALSE,
                      fig.path='figs/',
                      cache.path = '_cache/'#,
                      #fig.process = function(x) {
                      #x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
                      #if (file.rename(x, x2)) x2 else x
                      #}
                      )
```

# Background
## Mixture models
Finite mixture models can be represented by
$$
f(x; \Phi) = \sum_{j=1}^g \pi_j f_j(x; \theta_j)
$$
where $f(x; \Phi)$ is the probability density function (p.d.f.) or probability mass function
(p.m.f.) of the mixture model, $f_j(x; \theta_j)$ is the p.d.f. or p.m.f. of the $j$th
component of the mixture model, $\pi_j$ is the proportion of the $j$th component and
$\theta_j$ is the parameter of the $j$th component, which can be a scalar or a vector,
$\Phi$ is a vector of all the parameters of the mixture model. The maximum likelihood
estimate of $\Phi$ can be obtained by using the EM algorithm (Dempster *et al*, 1977).

## Selecting mixture models by BIC
One critical problem in mixture models is how to estimate $g$, the number of components in the mixture model. As EM algorithm doesn't estimate $g$ itself, a widely used approach to estimate $g$ is to fit a series of mixture models with different values of $g$ and select the best $g$ using some metrics such as Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), Deviance Information Criterion (DIC), Integrated Complete-data Likelihood (ICL), etc. Among all information criteria, BIC has shown to outperform other ones in model selection. BIC is defined as
$$
BIC = k \log(n) - 2 \log(\hat L)
$$
in which $k$ is the total number of parameters in the mixture model, $n$ is the size of data, and $\hat L$ is the maximum likelihood of the model. The model which has the lowest BIC value is regarded as an optimal one.

## Selecting mixture models by bootstrapping LRT
A mixture model with $g = g_1$ components is a nested model of a mixture model with $g = g_2(g_1 < g_2)$ components, as the former model can be regarded as the later one with $\pi_j = 0$ for $g_2 - g_1$ components and $p_j > 0$ for all the remaining $g_1$ components. The likelihood ratio test (LRT) is a common tool for assessing the goodness of the nested model ($H_0: g = g_1$) compared with the full model ($H_a: g = g_2$). However the regularity condition of the LRT, which requires that the parameter space of the model in the null hypothesis $H_0$ should lie in the interior of the parameter space of the model in the alternative hypothesis $H_a$, doesn't hold for the mixture models (need reference here), and therefore the test statistic of LRT, denoted as $\Lambda(x)$
doesn't follow a known Chi-square distribution under $H_0$. (Need reference here) proposed the idea of bootstrapping (needed reference here) to approximate the distribution of $\Lambda(x)$. The general
 step of bootstrap likelihood ratio test is as follows.


1. For the given data $x$, estimate $\Phi$ under both $H_0$ and $H_a$ to get $\hat\Phi_0$ and $\hat\Phi_1$. Calculate the observed log-likelihood $\ell(x; \hat\Phi_0)$ and $\ell(x; \hat\Phi_1)$. The LRT statistic is defined as
$w_0 = -2(\ell(x; \hat\Phi_0) - \ell(x; \hat\Phi_1)).$
2. Generate random data of the same size as the original data $x$ from the model under the $H_0$ using estimated parameter $\hat\Phi_0$, then repeat step 1 using the simulated data. Repeat this process for $B$ times to get a vector of the simulated likelihood ratio test statistics $w_1^{1}, \dots, w_1^{B}$.
3. Calculate the empirical p-value as
$$
p = \frac{1}{B} \sum_{i=1}^B I(w_1^{(i)} > w_0)
$$
where $I(\cdot)$ is the indicator function.
 

## Fitting mixture models to the binned data
The binned data is present sometimes instead of the raw data, for the reason of storage convenience or necessity. The binned data is recorded in the form of $(a_i, b_i, n_i)$ where $a_i$ is the lower bound of the $i$th bin, $b_i$ is the upper bound of the $i$th bin, and $n_i$ is the number of observations that fall in the $i$th bin, for $i = 1, \dots, r$, and $r$ is the total number of bins.

To obtain the maximum likelihood estimate of the finite mixture model for binned data, (need reference here)
introduce two types of latent variables $x$ and $z$, where $x$ represents the
value of the unknown raw data, and $z$ is a vector of zeros and exactly one value of one indicating the
component that $x$ belongs to. To use the EM algorithm we first write the complete-data
log-likelihood
$$
Q(\Phi; \Phi^{(p)}) = \sum_{j = 1}^{g} \sum_{i = 1}^r n_i z^{(p)} [\log f(x^{(p)}; \theta_j)
 + \log \pi_j ]
$$
where $z^{(p)}$ is the expected value of $z$ given the estimated value of $\Phi$ and expected value $x^{(p)}$ at $p$th iteration. The estimated value of $\Phi$ can be updated iteratively via the E-step, in which we estimate $\Phi$ by maximizing the complete-data loglikelihood, and M-step, in which we calculate the expected value of the latent variables $x$ and $z$. The EM algorithm is terminated by using a stopping rule. The M-step of the EM algorithm may or may not have closed-form solution (e.g. the Weibull mixture model or Gamma mixture model). If not, an iterative approach like Newton's algorithm or bisection method may be used.

## Beyond normality
The normal distribution is mostly used in a mixture model for continuous data, but there are also circumstances when other distributions fit the data better.  McLachlan and Peel (2004) explains a limitation of the normal mixture model is that when the subpopulations are skewed, there may not be a one-to-one correspondence between the number of subpopulations in the model and in the data. More than one normal component is needed to model a skewed component, which may cause overestimation. As an example, Weibull mixture models are commonly used in wind energy analysis, lifetime data, clinical trials etc. (Yu and Harvill, 2018).


# mixR package
We present the functions in **mixR** package for (a) fitting finite mixture models for continuous data for families of Normal, Weibull, Gamma and Lognormal, by using EM algorithm as well as Newton-Raphson algorithm and bisection method when necessary; (b) selecting an optimal number of components in a mixture model by using BIC or bootstrap LRT. We also discuss how to fit mixture models with binned data.

## Model fitting

The function `mixfit()` can be used to fit mixture models for four different families -- Normal, Weibull, Gamma, and Log-normal. For Normal distribution, we can constrain the variance of each component to be the same by setting `ev = TRUE`. The fitted results can be plotted using base R plot system or ggplot.
```{r, cache=TRUE, fig.show="hold", out.width="50%", fig.cap="Fitted normal mixture models with unequal variances (left) and equal variance(right)"}
library(mixR)

# generate data from a Normal mixture model
set.seed(102)
x1 = rmixnormal(1000, c(0.3, 0.7), c(-2, 3), c(2, 1))

# Normal mixture model (unequal variances)
mod1 = mixfit(x1, ncomp = 2); mod1

# Normal mixture model (equal variance)
mod1_ev = mixfit(x1, ncomp = 2, ev = TRUE); mod1_ev

plot(mod1, title = 'Normal Mixture (unequal variances)')
plot(mod1_ev, title = 'Normal Mixture (equal variance)')
```

The initial values for the parameters will be estimated by k-means or hierarchical clustering method if they are not provided manually by the users. In cases when the EM algorithm is stuck in a local minimum and leads to unsatisfactory fitting results, which could happen when the number of components in the mixture model is relatively large and/or the data size is large, initial values can be provided manually to get a better fitting.

Distributions such as Weilbull, Gamma, or Log-normal might provide better fitting than Normal distribution when the components in the mixture model are skewed, in which situations we can still fit normal mixture models to the data, but the risk is that the number of components are easily overestimated. 

To illustrate this, we simulate data from a Weibull mixture model with $g = 2$, then fit the data with both Normal and Weibull mixture models with the same $g$. It is obvious that Weibull provides a better fit by either checking the plots of the fitted results, or the fact that the log-likelihood of fitted Weibull mixture model is 463, much higher than that of the fitted Normal mixture model (389).

```{r, fig.show="hold", out.width="50%", cache=TRUE, eval=TRUE, fig.cap="The fitted Weibull mixture model (left) and Normal mixture model (right) to the same data"}
x2 = rmixweibull(1000, c(0.4, 0.6), c(0.6, 1.3), c(0.1, 0.1))
mod2_weibull = mixfit(x2, family = 'weibull', ncomp = 2)
mod2_normal = mixfit(x2, ncomp = 2)

mod2_weibull

mod2_normal

plot(mod2_weibull)
plot(mod2_normal)
```

To find out the best value of $g$ for the Weibull and Normal mixture models if we fit both of them to the data set `x2`, we can fit a series of mixture models with different values of $g$, and select the best $g$ using the information criterion BIC. This can be done by the function `select()` in the mixR package. From Figure 3 we see that the best Weibull mixture model has $g = 2$ and the best Normal mixture model has $g = 4$ with equal variance for each component. 
```{r, fig.show="hold", out.width="50%", cache=TRUE, eval=TRUE, fig.cap="The values of BIC for Weibull mixture models (left) and Normal mixture models (right) with different values of $g$. The model with the lowest value of BIC is regarded as the best."}
# Selecting the best g for Weibull mixture model
s_weibull = select(x2, ncomp = 2:6, family = 'weibull')
# Selecting the best g for Normal mixture model
s_normal = select(x2, ncomp = 2:6)
plot(s_weibull)
plot(s_normal)
```

```{r, fig.show="hold", out.width="50%", cache=TRUE, eval=TRUE, fig.cap="The fitted Weibull mixture model with $g = 2$ (left) and the Normal mixture model with $g = 4$ and equal variance (right)"}
plot(mod2_weibull)
plot(mixfit(x2, ncomp = 4, ev = TRUE))
```

## Model selection
The package mixR provides two methods to compare and select $g$ for mixture models. For the first method we fit a series of candidate mixture models with different values of $g$ and then select the best $g$ with the lowest BIC, which can be done by the function `select()` (mentioned above). For normal mixture models, both equal and unequal variances are studied. 

The second method is the bootstrap likelihood ratio test, which tests the null hypothesis $H_0:$ The data are from a mixture model with $g = g_1$ components vs. $H_1:$ The data are from a mixture model with $g = g_2$, where $g_1 < g_2$. The bootstrap likelihood ratio test can be done by the function `bs.test()`, which returns the p-value as well as the test statistic $w0$ and $w1$. As an example, the data set `x1` we generated above are from a Normal mixture with $g = 2$. If we test $g = 2$ against $g = 3$ for the data and set the bootstrap iterations `B=100`, we get a p-value of 0.38, showing that we are unable to reject the null hypothesis and therefore the Normal mixture model with three components is not any better than the one with two components to fit the data `x1`.

As another example, the data set `x2` above are generated from a Weibull mixture model with $g = 2$. We discussed previously that if we use Normal distribution to fit the mixture model, the best value for $g$ selected by BIC is 4. A bootstrap likelihood ratio test of $g = 2$ vs $g = 4$ gives us a p-value of 0, indicating that $g = 4$ is a much better fit than $g= 2$, though visually the data shows two modes rather than four.

```{r, fig.show="hold", out.width="50%", cache=TRUE, eval=TRUE, fig.cap="(left) The bootstrap likelihood ratio test of $H_0: g = 2$ vs. $H_1: g = 3$ for fitting a Normal mixture model for data `x1`; (right) The bootstrap likelihood ratio test of $H_0: g = 2$ vs. $H_1: g = 4$ for fitting a Normal mixture model for data `x2`"}
b1 = bs.test(x1, ncomp = c(2, 3))
str(b1)
plot(b1, main = 'Bootstrap Likelihood Ratio Test for 
     \n Normal Mixture Models (g = 2 vs g = 3)',
     xlab = "Data")

b2 = bs.test(x2, ncomp = c(2, 4))
str(b2)
plot(b2, main = 'Bootstrap Likelihood Ratio Test for 
     \n Normal Mixture Models  (g = 2 vs g = 4)',
     xlab = "Data")
```


## Mixture models fitted with binned data
It occurs that the data used to fit a mixture model don't have their original values available. This may happen because the mechanism that generates the data doesn't allow the original values to be collected or because of other reasons such as data transformation or compression. Mixture models in mixR package can also be fitted to the data without original values, in the format of binned data (or grouped data). To do so we just need to provide the function `mixfit()` with the binned data, which is a three-column matrix each row of which represents a bin with left bin value, right bin values, and the total number of data points that fall in the bin (exactly the same type of data used to create a histogram). The package contains a data set `Stamp2` which is binned data. Another data set `Stamp` is the simulated original data of `Stamp2` using the function `reinstate()`.

```{r, fig.show="hold", out.width="50%", cache=TRUE, eval=TRUE}
head(Stamp)
mod_stamp = mixfit(Stamp, ncomp = 3)
mod_stamp
plot(mod_stamp, title = 'Normal Mixture Model for the Original Data')

head(Stamp2)
mod_stamp2 = mixfit(Stamp2, ncomp = 3)
mod_stamp2
plot(mod_stamp2, title = 'Normal Mixture Model for the Binned Data')
```

mixR package provides two functions: `bin()` for converting original data to binned data, and `reinstate()` for simulating the original data with binned data. As binning is actually a way to compress data, fitting mixture models on binned data can accelerate the fitting process when the original data set is large.

To illustrate, we simulate 100,000 data points from a Normal mixture model with five components, and bin the data with 100 bins. Normal mixture models are fitted on both the simulated raw data and binned data. The results show that model fitting on raw data takes 27 seconds, and on binned data it only takes 0.95 seconds. Another example shows that fitting a Weibull mixture model with four components on data binned from a raw data set with one million observations takes just over two seconds!
```{r, fig.show="hold", out.width="50%", cache=TRUE, eval=TRUE}
generate_params = function(ncomp = 2) {
  pi = runif(ncomp)
  low = runif(1, 0, 0)
  upp = low + runif(1, 0, 10)
  mu = runif(ncomp, low, upp)
  sd = runif(ncomp, (max(mu) - min(mu)) / ncomp / 10, (max(mu) - min(mu)) / ncomp / 2)
  list(pi = pi / sum(pi), mu = sort(mu), sd = sd)
}

set.seed(988)
n = 100000
ncomp = 5
params = generate_params(ncomp)
x_large = rmixnormal(n, pi = params$pi, mu = params$mu, sd = params$sd)

# fitting mixture models on original data
t1 = Sys.time()
mod_large <- mixfit(x_large, ncomp = ncomp)
t2 = Sys.time()
t2 - t1

mod_large
plot(mod_large, title = 'Normal Mixture Model for Original Data')

# fitting mixture models on binned data
t3 = Sys.time()
x_binned = bin(x_large, seq(min(x_large), max(x_large), length = 100))
mod_binned <- mixfit(x_binned, ncomp = ncomp)
t4 = Sys.time()
t4 - t3

mod_binned
plot(mod_binned, title = 'Normal Mixture Model for Binned Data')
```

```{r, fig.show="hold", out.width="80%", cache=TRUE, eval=TRUE}
set.seed(247)
n = 1e6
ncomp = 4
params = generate_params(ncomp)
x_large_weibull = rmixweibull(n, pi = params$pi, mu = params$mu, sd = params$sd)
brks = seq(min(x_large_weibull), max(x_large_weibull), length = 100)
x_binned_weibull = bin(x_large_weibull, brks)

# fitting Weibull mixture models on binned data
t5 = Sys.time()
mod_binned_weibull <- mixfit(x_binned_weibull, ncomp = ncomp, family = 'weibull')
t6 = Sys.time()
t6 - t5

plot(mod_binned_weibull, title = 'Weibull Mixture Model for Binned Data')
```

```{r, echo=FALSE}
plot(2:5, type = 'n')
```
<!--
# References
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}
\noindent
-->
